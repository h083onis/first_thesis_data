{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import japanize_matplotlib\n",
    "from tqdm.notebook import tqdm\n",
    "from IPython.display import display\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# warning表示off\n",
    "import warnings\n",
    "warnings.simplefilter('ignore')\n",
    "\n",
    "# デフォルトフォントサイズ変更\n",
    "plt.rcParams['font.size'] = 14\n",
    "\n",
    "# デフォルトグラフサイズ変更\n",
    "plt.rcParams['figure.figsize'] = (6,6)\n",
    "\n",
    "# デフォルトで方眼表示ON\n",
    "plt.rcParams['axes.grid'] = True\n",
    "\n",
    "# numpyの表示桁数設定\n",
    "np.set_printoptions(suppress=True, precision=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data.dataset import Subset\n",
    "from sklearn.model_selection import KFold\n",
    "from torch.utils.data.sampler import WeightedRandomSampler\n",
    "from torch.nn.parallel import DistributedDataParallel as DDP\n",
    "import torch.distributed as dist\n",
    "import torch.multiprocessing as mp\n",
    "\n",
    "from learning_tool import *\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "\n",
    "from model.model import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "# デバイスの割り当て\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "# device = torch.device('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PyTorch乱数固定用\n",
    "\n",
    "def torch_seed(seed=123):\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.use_deterministic_algorithms = True\n",
    "    \n",
    "# 乱数初期化\n",
    "torch_seed()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class MyDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, df, features, labels):\n",
    "        self.features_values = df[features].values\n",
    "        self.labels = df[labels].values\n",
    "        \n",
    "    # len()を使用すると呼ばれる\n",
    "    def __len__(self):\n",
    "        return len(self.features_values)\n",
    "\n",
    "    # 要素を参照すると呼ばれる関数    \n",
    "    def __getitem__(self, idx):\n",
    "        features_x = torch.LongTensor(self.features_values[idx])\n",
    "        labels = torch.as_tensor(self.labels[idx])\n",
    "        return features_x, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_hidden: 100 n_output: 2\n"
     ]
    }
   ],
   "source": [
    "# 出力次元数\n",
    "# 分類先クラス数　今回は2になる\n",
    "n_output = 2\n",
    "\n",
    "# 隠れ層のノード数\n",
    "n_hidden = 100\n",
    "\n",
    "# 結果確認\n",
    "print(f'n_hidden: {n_hidden} n_output: {n_output}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CNN(\n",
      "  (relu): ReLU(inplace=True)\n",
      "  (embedding): Embedding(2001, 128)\n",
      "  (conv1): Conv2d(1, 128, kernel_size=(3, 128), stride=(1, 1))\n",
      "  (maxpool1): MaxPool2d(kernel_size=(1998, 1), stride=(1998, 1), padding=0, dilation=1, ceil_mode=False)\n",
      "  (conv2): Conv2d(1, 128, kernel_size=(4, 128), stride=(1, 1))\n",
      "  (maxpool2): MaxPool2d(kernel_size=(1997, 1), stride=(1997, 1), padding=0, dilation=1, ceil_mode=False)\n",
      "  (conv3): Conv2d(1, 128, kernel_size=(5, 128), stride=(1, 1))\n",
      "  (maxpool3): MaxPool2d(kernel_size=(1996, 1), stride=(1996, 1), padding=0, dilation=1, ceil_mode=False)\n",
      "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "  (l1): Linear(in_features=384, out_features=2, bias=True)\n",
      "  (dropout): Dropout(p=0.5, inplace=False)\n",
      "  (features1): Sequential(\n",
      "    (0): Conv2d(1, 128, kernel_size=(3, 128), stride=(1, 1))\n",
      "    (1): ReLU(inplace=True)\n",
      "    (2): MaxPool2d(kernel_size=(1998, 1), stride=(1998, 1), padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (features2): Sequential(\n",
      "    (0): Conv2d(1, 128, kernel_size=(4, 128), stride=(1, 1))\n",
      "    (1): ReLU(inplace=True)\n",
      "    (2): MaxPool2d(kernel_size=(1997, 1), stride=(1997, 1), padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (features3): Sequential(\n",
      "    (0): Conv2d(1, 128, kernel_size=(5, 128), stride=(1, 1))\n",
      "    (1): ReLU(inplace=True)\n",
      "    (2): MaxPool2d(kernel_size=(1996, 1), stride=(1996, 1), padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "net = CNN(n_output, n_hidden).to(device)\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_weighted_random_sampler(train_set):\n",
    "    numDataPoints = len(train_set)\n",
    "    data_dim = len(train_set[0][0])\n",
    "\n",
    "    data = torch.FloatTensor(numDataPoints, data_dim)\n",
    "    target = np.zeros(0)\n",
    "    target = (np.hstack(data[1].numpy() for data in train_set))\n",
    "    # target\n",
    "    # print(target)\n",
    "    print ('target train 0/1: {}/{}'.format(\n",
    "        len(np.where(target == 0)[0]), len(np.where(target == 1)[0])))\n",
    "\n",
    "    class_sample_count = np.array(\n",
    "        [len(np.where(target == t)[0]) for t in np.unique(target)])\n",
    "    weight = 1. / class_sample_count\n",
    "    samples_weight = np.array([weight[t] for t in target])\n",
    "\n",
    "    samples_weight = torch.from_numpy(samples_weight)\n",
    "    samples_weight = samples_weight.double()\n",
    "    # print(samples_weight)\n",
    "    sampler = WeightedRandomSampler(samples_weight, len(samples_weight), replacement=True)\n",
    "    return sampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_dataset(filename):\n",
    "    df = pd.read_csv(filename, index_col=0)\n",
    "    columns = df.columns.values\n",
    "    features_columns = columns[:-1]\n",
    "    labels_column = columns[-1]\n",
    "    dataset = MyDataset(df, features_columns, labels_column)\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def record_history(df, filename, seed, each_history, num_splits):\n",
    "    # print(df)\n",
    "    score_type = ['train_loss','train_acc','test_loss','test_acc','auc_score']\n",
    "    for num_kf in range(num_splits):\n",
    "        for i in range(5):\n",
    "            tmp_list = list(each_history[num_kf,:,i+1])\n",
    "            tmp_list[:0] = [filename, seed, num_kf+1, score_type[i]]\n",
    "            # print(tmp_list)\n",
    "            df.loc[len(df)] = tmp_list\n",
    "    # print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "expected an indented block (1401245152.py, line 13)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[12], line 13\u001b[0;36m\u001b[0m\n\u001b[0;31m    def configure_optimizers(self):\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m expected an indented block\n"
     ]
    }
   ],
   "source": [
    "# class BaseModel(pl.LightningModule):\n",
    "#     def __init__(self, model, classes, lr):\n",
    "#         super().__init__()\n",
    "#         self.model = model(num_classes=2, weights=None)\n",
    "#         self.loss = nn.CrossEntropyLoss()\n",
    "#         self.save_hyperparameters()\n",
    "        \n",
    "#     def forward(self, x):\n",
    "#         return self.model(x)\n",
    "    \n",
    "#     def training_step(self, batch):\n",
    "        \n",
    "#     def configure_optimizers(self):\n",
    "#         return optim.Adam(self.parameters(), lr=lr=self.hparams.lr, weight_decay=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "txt_vec_and_label_camel_10.csv\n",
      "100\n",
      "cv: 1\n",
      "target train 0/1: 14699/8626\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Error initializing torch.distributed using env:// rendezvous: environment variable RANK expected, but not set",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 48\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[38;5;66;03m# モデルインスタンス生成\u001b[39;00m\n\u001b[1;32m     47\u001b[0m net \u001b[38;5;241m=\u001b[39m CNN(n_output, n_hidden)\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m---> 48\u001b[0m \u001b[43mdist\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minit_process_group\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbackend\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mnccl\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     49\u001b[0m model \u001b[38;5;241m=\u001b[39m DDP(model, device_ids\u001b[38;5;241m=\u001b[39m[torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mcurrent_device()])\n\u001b[1;32m     51\u001b[0m \u001b[38;5;66;03m# 最適化関数: 勾配降下法\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/distributed/distributed_c10d.py:888\u001b[0m, in \u001b[0;36minit_process_group\u001b[0;34m(backend, init_method, timeout, world_size, rank, store, group_name, pg_options)\u001b[0m\n\u001b[1;32m    884\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m store \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    885\u001b[0m     rendezvous_iterator \u001b[38;5;241m=\u001b[39m rendezvous(\n\u001b[1;32m    886\u001b[0m         init_method, rank, world_size, timeout\u001b[38;5;241m=\u001b[39mtimeout\n\u001b[1;32m    887\u001b[0m     )\n\u001b[0;32m--> 888\u001b[0m     store, rank, world_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mrendezvous_iterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    889\u001b[0m     store\u001b[38;5;241m.\u001b[39mset_timeout(timeout)\n\u001b[1;32m    891\u001b[0m     \u001b[38;5;66;03m# Use a PrefixStore to avoid accidental overrides of keys used by\u001b[39;00m\n\u001b[1;32m    892\u001b[0m     \u001b[38;5;66;03m# different systems (e.g. RPC) in case the store is multi-tenant.\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/distributed/rendezvous.py:235\u001b[0m, in \u001b[0;36m_env_rendezvous_handler\u001b[0;34m(url, timeout, **kwargs)\u001b[0m\n\u001b[1;32m    233\u001b[0m     rank \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(query_dict[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrank\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m    234\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 235\u001b[0m     rank \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(\u001b[43m_get_env_or_raise\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mRANK\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    237\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mworld_size\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m query_dict:\n\u001b[1;32m    238\u001b[0m     world_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(query_dict[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mworld_size\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/distributed/rendezvous.py:220\u001b[0m, in \u001b[0;36m_env_rendezvous_handler.<locals>._get_env_or_raise\u001b[0;34m(env_var)\u001b[0m\n\u001b[1;32m    218\u001b[0m env_val \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39menviron\u001b[38;5;241m.\u001b[39mget(env_var, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m    219\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m env_val:\n\u001b[0;32m--> 220\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m _env_error(env_var)\n\u001b[1;32m    221\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    222\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m env_val\n",
      "\u001b[0;31mValueError\u001b[0m: Error initializing torch.distributed using env:// rendezvous: environment variable RANK expected, but not set"
     ]
    }
   ],
   "source": [
    "# 学習率\n",
    "lr = 0.0001\n",
    "\n",
    "# 繰り返し回数\n",
    "num_epochs = 50\n",
    "\n",
    "# 評価結果記録用\n",
    "cv_history = np.zeros((0,6))\n",
    "\n",
    "batch_size = 66\n",
    "num_splits = 10\n",
    "\n",
    "columns_list = [i for i in range(1,num_epochs+1)]\n",
    "columns_list[:0] = ['filename','seed', 'num_kf', 'score_type']\n",
    "columns_list\n",
    "df = pd.DataFrame(columns=columns_list)\n",
    "\n",
    "for i in range(10,11):\n",
    "    filename = 'txt_vec_and_label_camel_'+str(i)+'.csv'\n",
    "    copy_df = df.copy()\n",
    "    path = '../resource/' + filename\n",
    "    dataset = make_dataset(path)\n",
    "    print(filename)\n",
    "    \n",
    "    random_seed_list = [100,101,102,103,104,105,106,107,108,109,110]\n",
    "    # random_seed_list = [100]\n",
    "    # random_seed_list = [102,103,104,105,106,107,108,109]\n",
    "    \n",
    "    for seed in random_seed_list:\n",
    "        print(seed)\n",
    "        kf = KFold(n_splits=num_splits, shuffle=True, random_state=seed)\n",
    "        cv_cnt = 0\n",
    "        torch_seed(seed)\n",
    "        each_history = np.zeros((0,num_epochs,6))\n",
    "\n",
    "        for train_index, test_index in kf.split(dataset):\n",
    "            cv_cnt += 1\n",
    "            print(f'cv: {cv_cnt}')\n",
    "            history = np.zeros((0,6))\n",
    "            train_dataset = Subset(dataset, train_index)\n",
    "            sampler = make_weighted_random_sampler(train_dataset)\n",
    "            train_loader = DataLoader(train_dataset, batch_size, sampler=sampler, num_workers=2)\n",
    "            test_dataset   = Subset(dataset, test_index)\n",
    "            test_loader = DataLoader(test_dataset, batch_size, shuffle=False, num_workers=2)\n",
    "            \n",
    "            # モデルインスタンス生成\n",
    "            net = CNN(n_output, n_hidden).to(device)\n",
    "            dist.init_process_group(backend='nccl')\n",
    "            model = DDP(model, device_ids=[torch.cuda.current_device()])\n",
    "            \n",
    "            # 最適化関数: 勾配降下法\n",
    "            optimizer = optim.Adam(net.parameters(), lr=lr, weight_decay=0.001)\n",
    "            # optimizer = torch.optim.SGD(net.parameters(), lr=lr)\n",
    "            # 損失関数： 交差エントロピー関数\n",
    "            criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "            #学習\n",
    "            history = fit(net, optimizer, criterion, num_epochs, train_loader, test_loader, device, history, test_dataset)\n",
    "\n",
    "            #1交差ごとの記録\n",
    "            each_history = np.vstack((each_history, [history]))\n",
    "            \n",
    "        record_history(copy_df, filename, seed, each_history, num_splits)\n",
    "        print(copy_df)\n",
    "    copy_df.to_csv('../result/threshold_2000_epoch_50_cv_10_2_weighted_random_camel_'+str(i)+'.csv', mode='w',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# each_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# copy_df.to_csv('../result/threshold_2000_epoch_50_cv_10_seed_101_weighted_random_camel_'+str(i)+'.csv', mode='w',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "evaluate_history(each_history[0,:,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# np.save('../result/camel_3_cv_5_epoch_10.npy',each_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# np.save('../result/camel_3_cv_5_cv_hisotry.npy', cv_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.save(net.state_dict(), '../model_weight.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cv_history = np.zeros((0,0,5))\n",
    "# cv_history.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# cv_history = np.zeros((0,5))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# cv_cnt = 1\n",
    "# num_epoch = 10\n",
    "# each_history = np.zeros((0,num_epoch,5))\n",
    "# history = np.zeros((0,5))\n",
    "# item = np.arange(5).reshape(1,5)\n",
    "# history = np.vstack((history, item))\n",
    "# history\n",
    "# history = np.arange(num_epoch*5).reshape((num_epoch,5))\n",
    "# each_history = np.vstack((each_history, [history]))\n",
    "# each_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# history = np.arange(num_epoch*5).reshape((num_epoch,5))\n",
    "# each_history = np.vstack((each_history, [history]))\n",
    "# each_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# history = np.zeros((0,5))\n",
    "# history"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
